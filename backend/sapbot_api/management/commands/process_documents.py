# backend/sapbot_api/management/commands/process_documents.py
"""
SAPBot API Document Processing Management Command
DÃ¶kÃ¼manlarÄ± toplu olarak iÅŸleme, embedding oluÅŸturma ve optimize etme
"""
from django.core.management.base import BaseCommand, CommandError
from django.utils import timezone
from django.db import models
from datetime import timedelta
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
# SAPBot models
from sapbot_api.models import DocumentSource, KnowledgeChunk, ErrorLog
from sapbot_api.services.document_processor import DocumentProcessor
from sapbot_api.services.embedding_service import EmbeddingService
from sapbot_api.utils.file_handlers import FileValidator, PDFProcessor, VideoProcessor
from sapbot_api.utils.text_processing import TextAnalyzer, TextChunker
from sapbot_api.utils.exceptions import DocumentProcessingError
from sapbot_api.utils.helpers import format_file_size
from sapbot_api.tasks.document_tasks import process_document_async

logger = logging.getLogger('sapbot_api.management')

class Command(BaseCommand):
    """
    SAPBot dÃ¶kÃ¼man iÅŸleme komutu
    
    Usage examples:
    python manage.py process_documents --status pending --limit 10
    python manage.py process_documents --reprocess --document-id abc123
    python manage.py process_documents --batch --workers 4
    python manage.py process_documents --cleanup --days 7
    python manage.py process_documents --rebuild-embeddings --force
    """
    
    help = 'SAPBot dÃ¶kÃ¼manlarÄ± iÅŸler, embedding oluÅŸturur ve optimize eder'
    
    def add_arguments(self, parser):
        """Command line arguments"""
        parser.add_argument(
            '--status',
            type=str,
            choices=['pending', 'processing', 'completed', 'failed', 'all'],
            default='pending',
            help='Ä°ÅŸlenecek dÃ¶kÃ¼man durumu'
        )
        
        parser.add_argument(
            '--document-id',
            type=str,
            default=None,
            help='Belirli bir dÃ¶kÃ¼manÄ± iÅŸle (UUID)'
        )
        
        parser.add_argument(
            '--document-type',
            type=str,
            choices=['pdf', 'video', 'manual', 'api', 'web', 'all'],
            default='all',
            help='Ä°ÅŸlenecek dÃ¶kÃ¼man tipi'
        )
        
        parser.add_argument(
            '--limit',
            type=int,
            default=None,
            help='Ä°ÅŸlenecek maksimum dÃ¶kÃ¼man sayÄ±sÄ±'
        )
        
        parser.add_argument(
            '--batch',
            action='store_true',
            help='Batch mode - birden fazla dÃ¶kÃ¼manÄ± paralel iÅŸle'
        )
        
        parser.add_argument(
            '--workers',
            type=int,
            default=2,
            help='Paralel worker sayÄ±sÄ± (batch mode iÃ§in)'
        )
        
        parser.add_argument(
            '--reprocess',
            action='store_true',
            help='Daha Ã¶nce iÅŸlenmiÅŸ dÃ¶kÃ¼manlarÄ± yeniden iÅŸle'
        )
        
        parser.add_argument(
            '--force',
            action='store_true',
            help='HatalarÄ± gÃ¶rmezden gel ve iÅŸlemeye devam et'
        )
        
        parser.add_argument(
            '--cleanup',
            action='store_true',
            help='Eski ve bozuk dÃ¶kÃ¼man kayÄ±tlarÄ±nÄ± temizle'
        )
        
        parser.add_argument(
            '--days',
            type=int,
            default=30,
            help='Cleanup iÃ§in gÃ¼n sayÄ±sÄ± (default: 30)'
        )
        
        parser.add_argument(
            '--rebuild-embeddings',
            action='store_true',
            help='TÃ¼m embedding\'leri yeniden oluÅŸtur'
        )
        
        parser.add_argument(
            '--chunk-size',
            type=int,
            default=1000,
            help='Metin chunk boyutu'
        )
        
        parser.add_argument(
            '--chunk-overlap',
            type=int,
            default=200,
            help='Chunk overlap boyutu'
        )
        
        parser.add_argument(
            '--dry-run',
            action='store_true',
            help='Sadece analiz yap, iÅŸleme yapma'
        )
        
        parser.add_argument(
            '--verbose',
            action='store_true',
            help='DetaylÄ± log Ã§Ä±ktÄ±sÄ±'
        )
        
        parser.add_argument(
            '--async-mode',
            action='store_true',
            help='Celery task\'lar ile asenkron iÅŸleme'
        )
        
        parser.add_argument(
            '--optimize',
            action='store_true',
            help='DÃ¶kÃ¼man optimizasyonu yap (duplicate removal, etc.)'
        )
    
    def handle(self, *args, **options):
        """Ana iÅŸleme metodu"""
        try:
            # BaÅŸlangÄ±Ã§ mesajÄ±
            self.stdout.write(
                self.style.HTTP_INFO(f"ğŸš€ SAPBot Document Processing baÅŸlatÄ±lÄ±yor...")
            )
            
            # Options validation
            self._validate_options(options)
            
            # Initialize services
            self.document_processor = DocumentProcessor()
            self.embedding_service = EmbeddingService()
            self.file_validator = FileValidator()
            
            # Performance tracking
            start_time = time.time()
            
            # Ana iÅŸlem seÃ§imi
            if options['cleanup']:
                result = self._perform_cleanup(options)
            elif options['rebuild_embeddings']:
                result = self._rebuild_embeddings(options)
            elif options['optimize']:
                result = self._optimize_documents(options)
            else:
                result = self._process_documents(options)
            
            # SonuÃ§ raporu
            end_time = time.time()
            duration = end_time - start_time
            self._display_results(result, duration, options)
            
        except Exception as e:
            logger.error(f"Document processing hatasÄ±: {e}")
            raise CommandError(f"Ä°ÅŸlem baÅŸarÄ±sÄ±z: {str(e)}")
    
    def _validate_options(self, options):
        """Options doÄŸrulama"""
        # Worker count validation
        if options['workers'] < 1 or options['workers'] > 10:
            raise CommandError("Worker sayÄ±sÄ± 1-10 arasÄ±nda olmalÄ±dÄ±r")
        
        # Chunk size validation
        if options['chunk_size'] < 100 or options['chunk_size'] > 5000:
            raise CommandError("Chunk size 100-5000 arasÄ±nda olmalÄ±dÄ±r")
        
        # Document ID validation
        if options['document_id']:
            try:
                from uuid import UUID
                UUID(options['document_id'])
            except ValueError:
                raise CommandError("GeÃ§ersiz document ID formatÄ±")
        
        # Conflicting options check
        if options['cleanup'] and options['rebuild_embeddings']:
            raise CommandError("Cleanup ve rebuild-embeddings aynÄ± anda kullanÄ±lamaz")
    
    def _process_documents(self, options):
        """Ana dÃ¶kÃ¼man iÅŸleme"""
        # Documents queryset oluÅŸtur
        queryset = self._build_documents_queryset(options)
        
        if options['dry_run']:
            return self._perform_dry_run(queryset, options)
        
        # Processing mode seÃ§imi
        if options['async_mode']:
            return self._process_documents_async(queryset, options)
        elif options['batch']:
            return self._process_documents_batch(queryset, options)
        else:
            return self._process_documents_sequential(queryset, options)
    
    def _build_documents_queryset(self, options):
        """Ä°ÅŸlenecek dÃ¶kÃ¼manlarÄ±n queryset'ini oluÅŸtur"""
        queryset = DocumentSource.objects.all()
        
        # Status filtering
        if options['status'] != 'all':
            if options['reprocess'] and options['status'] == 'pending':
                # Reprocess mode'da completed ve failed olanlarÄ± da al
                queryset = queryset.filter(
                    processing_status__in=['pending', 'completed', 'failed']
                )
            else:
                queryset = queryset.filter(processing_status=options['status'])
        
        # Document type filtering
        if options['document_type'] != 'all':
            queryset = queryset.filter(document_type=options['document_type'])
        
        # Specific document
        if options['document_id']:
            queryset = queryset.filter(id=options['document_id'])
        
        # Ordering
        queryset = queryset.order_by('created_at')
        
        # Limit
        if options['limit']:
            queryset = queryset[:options['limit']]
        
        return queryset.select_related('uploaded_by')
    
    def _perform_dry_run(self, queryset, options):
        """Dry run - analiz yap"""
        self.stdout.write(self.style.WARNING("ğŸ” DRY RUN - Sadece analiz yapÄ±lÄ±yor..."))
        
        total_docs = queryset.count()
        
        # Status breakdown
        status_breakdown = {}
        for status in ['pending', 'processing', 'completed', 'failed']:
            count = queryset.filter(processing_status=status).count()
            if count > 0:
                status_breakdown[status] = count
        
        # Type breakdown
        type_breakdown = {}
        for doc_type in ['pdf', 'video', 'manual', 'api', 'web']:
            count = queryset.filter(document_type=doc_type).count()
            if count > 0:
                type_breakdown[doc_type] = count
        
        # Size analysis
        total_size = 0
        size_breakdown = {'small': 0, 'medium': 0, 'large': 0}
        
        for doc in queryset:
            if doc.file_size:
                total_size += doc.file_size
                size_mb = doc.file_size / (1024 * 1024)
                if size_mb < 10:
                    size_breakdown['small'] += 1
                elif size_mb < 50:
                    size_breakdown['medium'] += 1
                else:
                    size_breakdown['large'] += 1
        
        # Chunk estimation
        estimated_chunks = 0
        for doc in queryset[:100]:  # Sample ilk 100 dÃ¶kÃ¼man
            if doc.document_type == 'pdf' and doc.file_size:
                # PDF iÃ§in page estimate
                estimated_pages = max(1, doc.file_size // (1024 * 50))  # ~50KB per page
                estimated_chunks += estimated_pages * 2  # ~2 chunk per page
            elif doc.document_type == 'manual' and hasattr(doc, 'content'):
                # Manual content iÃ§in estimate
                content_length = len(getattr(doc, 'content', ''))
                estimated_chunks += max(1, content_length // options['chunk_size'])
        
        # Results
        self.stdout.write(f"\nğŸ“Š Ä°ÅŸleme Analizi:")
        self.stdout.write(f"  â€¢ Toplam dÃ¶kÃ¼man: {total_docs:,}")
        self.stdout.write(f"  â€¢ Toplam boyut: {format_file_size(total_size)}")
        
        if status_breakdown:
            self.stdout.write(f"\nğŸ“ˆ Status DaÄŸÄ±lÄ±mÄ±:")
            for status, count in status_breakdown.items():
                self.stdout.write(f"  â€¢ {status}: {count:,}")
        
        if type_breakdown:
            self.stdout.write(f"\nğŸ“„ Tip DaÄŸÄ±lÄ±mÄ±:")
            for doc_type, count in type_breakdown.items():
                self.stdout.write(f"  â€¢ {doc_type}: {count:,}")
        
        if any(size_breakdown.values()):
            self.stdout.write(f"\nğŸ’¾ Boyut DaÄŸÄ±lÄ±mÄ±:")
            self.stdout.write(f"  â€¢ KÃ¼Ã§Ã¼k (<10MB): {size_breakdown['small']:,}")
            self.stdout.write(f"  â€¢ Orta (10-50MB): {size_breakdown['medium']:,}")
            self.stdout.write(f"  â€¢ BÃ¼yÃ¼k (>50MB): {size_breakdown['large']:,}")
        
        self.stdout.write(f"\nğŸ§© Tahmini chunk sayÄ±sÄ±: {estimated_chunks:,}")
        
        # Processing time estimation
        if options['batch']:
            estimated_time = total_docs * 30 / options['workers']  # 30 sec per doc per worker
        else:
            estimated_time = total_docs * 45  # 45 sec per doc sequential
        
        self.stdout.write(f"â±ï¸ Tahmini iÅŸlem sÃ¼resi: {estimated_time/60:.1f} dakika")
        
        return {
            'total_documents': total_docs,
            'status_breakdown': status_breakdown,
            'type_breakdown': type_breakdown,
            'size_breakdown': size_breakdown,
            'estimated_chunks': estimated_chunks,
            'estimated_time': estimated_time,
            'dry_run': True
        }
    
    def _process_documents_sequential(self, queryset, options):
        """SÄ±ralÄ± dÃ¶kÃ¼man iÅŸleme"""
        total_docs = queryset.count()
        processed = 0
        succeeded = 0
        failed = 0
        chunks_created = 0
        errors = []
        
        self.stdout.write(f"ğŸ“„ {total_docs} dÃ¶kÃ¼man sÄ±ralÄ± iÅŸleniyor...")
        
        for i, document in enumerate(queryset, 1):
            try:
                if options['verbose']:
                    self.stdout.write(f"  [{i}/{total_docs}] Ä°ÅŸleniyor: {document.title}")
                
                # Process document
                result = self._process_single_document(document, options)
                
                if result['success']:
                    succeeded += 1
                    chunks_created += result.get('chunks_created', 0)
                    if options['verbose']:
                        self.stdout.write(
                            self.style.SUCCESS(f"    âœ… BaÅŸarÄ±lÄ±: {result.get('chunks_created', 0)} chunk")
                        )
                else:
                    failed += 1
                    errors.append({
                        'document': document.title,
                        'error': result.get('error', 'Unknown error')
                    })
                    if options['verbose']:
                        self.stdout.write(
                            self.style.ERROR(f"    âŒ BaÅŸarÄ±sÄ±z: {result.get('error', '')}")
                        )
                
                processed += 1
                
                # Progress indicator
                if not options['verbose'] and i % 10 == 0:
                    progress = (i / total_docs) * 100
                    self.stdout.write(f"  Ä°lerleme: {progress:.1f}% ({i}/{total_docs})")
                
            except Exception as e:
                failed += 1
                error_msg = str(e)
                errors.append({
                    'document': document.title,
                    'error': error_msg
                })
                
                if not options['force']:
                    logger.error(f"Document processing hatasÄ±: {e}")
                    break
                
                if options['verbose']:
                    self.stdout.write(
                        self.style.ERROR(f"    âŒ Hata: {error_msg}")
                    )
        
        return {
            'total_documents': total_docs,
            'processed': processed,
            'succeeded': succeeded,
            'failed': failed,
            'chunks_created': chunks_created,
            'errors': errors,
            'mode': 'sequential'
        }
    
    def _process_documents_batch(self, queryset, options):
        """Paralel batch dÃ¶kÃ¼man iÅŸleme"""
        total_docs = queryset.count()
        workers = options['workers']
        
        self.stdout.write(f"âš¡ {total_docs} dÃ¶kÃ¼man {workers} worker ile paralel iÅŸleniyor...")
        
        processed = 0
        succeeded = 0
        failed = 0
        chunks_created = 0
        errors = []
        
        # ThreadPoolExecutor ile paralel iÅŸleme
        with ThreadPoolExecutor(max_workers=workers) as executor:
            # Submit all documents
            future_to_doc = {
                executor.submit(self._process_single_document, doc, options): doc
                for doc in queryset
            }
            
            # Process results as they complete
            for i, future in enumerate(as_completed(future_to_doc), 1):
                document = future_to_doc[future]
                
                try:
                    result = future.result()
                    
                    if result['success']:
                        succeeded += 1
                        chunks_created += result.get('chunks_created', 0)
                        if options['verbose']:
                            self.stdout.write(
                                self.style.SUCCESS(
                                    f"  [{i}/{total_docs}] âœ… {document.title}: {result.get('chunks_created', 0)} chunk"
                                )
                            )
                    else:
                        failed += 1
                        errors.append({
                            'document': document.title,
                            'error': result.get('error', 'Unknown error')
                        })
                        if options['verbose']:
                            self.stdout.write(
                                self.style.ERROR(
                                    f"  [{i}/{total_docs}] âŒ {document.title}: {result.get('error', '')}"
                                )
                            )
                    
                    processed += 1
                    
                    # Progress indicator
                    if not options['verbose'] and i % 10 == 0:
                        progress = (i / total_docs) * 100
                        self.stdout.write(f"  Ä°lerleme: {progress:.1f}% ({i}/{total_docs})")
                
                except Exception as e:
                    failed += 1
                    error_msg = str(e)
                    errors.append({
                        'document': document.title,
                        'error': error_msg
                    })
                    
                    if options['verbose']:
                        self.stdout.write(
                            self.style.ERROR(f"  [{i}/{total_docs}] âŒ {document.title}: {error_msg}")
                        )
        
        return {
            'total_documents': total_docs,
            'processed': processed,
            'succeeded': succeeded,
            'failed': failed,
            'chunks_created': chunks_created,
            'errors': errors,
            'mode': f'batch (workers: {workers})'
        }
    
    def _process_documents_async(self, queryset, options):
        """Asenkron Celery task ile iÅŸleme"""
        total_docs = queryset.count()
        
        self.stdout.write(f"ğŸ”„ {total_docs} dÃ¶kÃ¼man Celery task ile asenkron iÅŸleniyor...")
        
        task_ids = []
        
        # Submit tasks
        for document in queryset:
            task = process_document_async.delay(
                document.id,
                chunk_size=options['chunk_size'],
                chunk_overlap=options['chunk_overlap'],
                force_reprocess=options['reprocess']
            )
            task_ids.append(task.id)
            
            if options['verbose']:
                self.stdout.write(f"  ğŸ“¤ Task submitted: {document.title} (task_id: {task.id})")
        
        self.stdout.write(f"âœ… {len(task_ids)} task Celery kuyruÄŸuna gÃ¶nderildi")
        self.stdout.write(f"ğŸ” Task durumunu ÅŸu komutla takip edebilirsiniz:")
        self.stdout.write(f"   celery -A sapreports inspect active")
        
        return {
            'total_documents': total_docs,
            'tasks_submitted': len(task_ids),
            'task_ids': task_ids,
            'mode': 'async (Celery)'
        }
    
    def _process_single_document(self, document, options):
        """Tek dÃ¶kÃ¼manÄ± iÅŸle"""
        try:
            # Status update
            document.processing_status = 'processing'
            document.save(update_fields=['processing_status'])
            
            # Document type'a gÃ¶re iÅŸleme
            if document.document_type == 'pdf':
                result = self._process_pdf_document(document, options)
            elif document.document_type == 'video':
                result = self._process_video_document(document, options)
            elif document.document_type == 'manual':
                result = self._process_manual_document(document, options)
            else:
                raise DocumentProcessingError(f"Desteklenmeyen dÃ¶kÃ¼man tipi: {document.document_type}")
            
            # Success update
            document.processing_status = 'completed'
            document.processed_at = timezone.now()
            document.processing_error = None
            document.save(update_fields=['processing_status', 'processed_at', 'processing_error'])
            
            return {
                'success': True,
                'chunks_created': result.get('chunks_created', 0),
                'processing_time': result.get('processing_time', 0)
            }
            
        except Exception as e:
            # Error update
            error_msg = str(e)
            document.processing_status = 'failed'
            document.processing_error = error_msg
            document.save(update_fields=['processing_status', 'processing_error'])
            
            # Error log
            ErrorLog.objects.create(
                error_type='document_processing_error',
                error_level='high',
                error_message=error_msg,
                context={
                    'document_id': str(document.id),
                    'document_title': document.title,
                    'document_type': document.document_type,
                }
            )
            
            return {
                'success': False,
                'error': error_msg
            }
    
    def _process_pdf_document(self, document, options):
        """PDF dÃ¶kÃ¼man iÅŸleme"""
        if not document.file_path:
            raise DocumentProcessingError("PDF dosya yolu bulunamadÄ±")
        
        start_time = time.time()
        
        # PDF processor ile metin Ã§Ä±kar
        pdf_processor = PDFProcessor()
        file_path = document.file_path.path
        
        extract_result = pdf_processor.extract_text(file_path)
        
        # Text chunking
        text_chunker = TextChunker()
        chunks_created = 0
        
        for page_data in extract_result['text_content']:
            page_text = page_data['text']
            page_number = page_data['page_number']
            
            if not page_text.strip():
                continue
            
            # Chunk'lara bÃ¶l
            chunks = text_chunker.intelligent_split(
                page_text,
                max_chunk_size=options['chunk_size'],
                overlap=options['chunk_overlap'],
                preserve_paragraphs=True
            )
            
            # Chunk'larÄ± kaydet
            for chunk in chunks:
                # Text analysis
                analysis = TextAnalyzer.analyze_text(chunk.text)
                
                # Embedding generate
                embedding = self.embedding_service.generate_embedding(chunk.text)
                
                # Knowledge chunk oluÅŸtur
                knowledge_chunk = KnowledgeChunk.objects.create(
                    source=document,
                    content=chunk.text,
                    content_length=chunk.char_count,
                    embedding=embedding,
                    embedding_model=self.embedding_service.model_name,
                    page_number=page_number,
                    sap_module=analysis.sap_modules[0] if analysis.sap_modules else None,
                    technical_level=analysis.technical_level,
                    keywords=analysis.keywords,
                    relevance_score=1.0,
                )
                
                chunks_created += 1
        
        processing_time = time.time() - start_time
        
        return {
            'chunks_created': chunks_created,
            'processing_time': processing_time,
            'pages_processed': len(extract_result['text_content'])
        }
    
    def _process_video_document(self, document, options):
        """Video dÃ¶kÃ¼man iÅŸleme"""
        if not document.file_path:
            raise DocumentProcessingError("Video dosya yolu bulunamadÄ±")
        
        start_time = time.time()
        
        # Video processor ile transkript Ã§Ä±kar
        video_processor = VideoProcessor()
        file_path = document.file_path.path
        
        transcript_result = video_processor.extract_transcript(file_path)
        
        # Text chunking
        text_chunker = TextChunker()
        chunks_created = 0
        
        full_text = transcript_result['full_text']
        
        if full_text.strip():
            # Chunk'lara bÃ¶l
            chunks = text_chunker.intelligent_split(
                full_text,
                max_chunk_size=options['chunk_size'],
                overlap=options['chunk_overlap'],
                preserve_sentences=True
            )
            
            # Chunk'larÄ± kaydet
            for chunk in chunks:
                # Text analysis
                analysis = TextAnalyzer.analyze_text(chunk.text)
                
                # Embedding generate
                embedding = self.embedding_service.generate_embedding(chunk.text)
                
                # Knowledge chunk oluÅŸtur
                knowledge_chunk = KnowledgeChunk.objects.create(
                    source=document,
                    content=chunk.text,
                    content_length=chunk.char_count,
                    embedding=embedding,
                    embedding_model=self.embedding_service.model_name,
                    sap_module=analysis.sap_modules[0] if analysis.sap_modules else None,
                    technical_level=analysis.technical_level,
                    keywords=analysis.keywords,
                    relevance_score=1.0,
                )
                
                chunks_created += 1
        
        processing_time = time.time() - start_time
        
        return {
            'chunks_created': chunks_created,
            'processing_time': processing_time,
            'video_duration': transcript_result['duration']
        }
    
    def _process_manual_document(self, document, options):
        """Manuel dÃ¶kÃ¼man iÅŸleme"""
        # Manuel dÃ¶kÃ¼manlar iÃ§in content field'Ä± olmalÄ±
        if not hasattr(document, 'content') or not document.content:
            raise DocumentProcessingError("Manuel dÃ¶kÃ¼man iÃ§eriÄŸi bulunamadÄ±")
        
        start_time = time.time()
        
        # Text chunking
        text_chunker = TextChunker()
        chunks_created = 0
        
        content = document.content
        
        # Chunk'lara bÃ¶l
        chunks = text_chunker.intelligent_split(
            content,
            max_chunk_size=options['chunk_size'],
            overlap=options['chunk_overlap'],
            preserve_paragraphs=True
        )
        
        # Chunk'larÄ± kaydet
        for chunk in chunks:
            # Text analysis
            analysis = TextAnalyzer.analyze_text(chunk.text)
            
            # Embedding generate
            embedding = self.embedding_service.generate_embedding(chunk.text)
            
            # Knowledge chunk oluÅŸtur
            knowledge_chunk = KnowledgeChunk.objects.create(
                source=document,
                content=chunk.text,
                content_length=chunk.char_count,
                embedding=embedding,
                embedding_model=self.embedding_service.model_name,
                sap_module=analysis.sap_modules[0] if analysis.sap_modules else None,
                technical_level=analysis.technical_level,
                keywords=analysis.keywords,
                relevance_score=1.0,
            )
            
            chunks_created += 1
        
        processing_time = time.time() - start_time
        
        return {
            'chunks_created': chunks_created,
            'processing_time': processing_time,
            'content_length': len(content)
        }
    
    def _perform_cleanup(self, options):
        """Eski ve bozuk dÃ¶kÃ¼manlarÄ± temizle"""
        days = options['days']
        cutoff_date = timezone.now() - timedelta(days=days)
        
        self.stdout.write(f"ğŸ§¹ {days} gÃ¼nden eski dÃ¶kÃ¼manlar temizleniyor...")
        
        # Failed documents
        failed_docs = DocumentSource.objects.filter(
            processing_status='failed',
            created_at__lt=cutoff_date
        )
        failed_count = failed_docs.count()
        
        # Orphaned chunks
        orphaned_chunks = KnowledgeChunk.objects.filter(
            source__isnull=True
        )
        orphaned_count = orphaned_chunks.count()
        
        # Empty documents
        empty_docs = DocumentSource.objects.filter(
            chunks__isnull=True,
            processing_status='completed',
            created_at__lt=cutoff_date
        )
        empty_count = empty_docs.count()
        
        if options['dry_run']:
            self.stdout.write(f"  ğŸ“Š Temizlenecek veriler:")
            self.stdout.write(f"    â€¢ BaÅŸarÄ±sÄ±z dÃ¶kÃ¼manlar: {failed_count}")
            self.stdout.write(f"    â€¢ Yetim chunk'lar: {orphaned_count}")
            self.stdout.write(f"    â€¢ BoÅŸ dÃ¶kÃ¼manlar: {empty_count}")
            return {
                'failed_documents': failed_count,
                'orphaned_chunks': orphaned_count,
                'empty_documents': empty_count,
                'dry_run': True
            }
        
        # Actual cleanup
        deleted_counts = {}
        
        if failed_count > 0:
            deleted_counts['failed_documents'] = failed_docs.delete()[0]
            self.stdout.write(f"  âœ… {deleted_counts['failed_documents']} baÅŸarÄ±sÄ±z dÃ¶kÃ¼man silindi")
        
        if orphaned_count > 0:
            deleted_counts['orphaned_chunks'] = orphaned_chunks.delete()[0]
            self.stdout.write(f"  âœ… {deleted_counts['orphaned_chunks']} yetim chunk silindi")
        
        if empty_count > 0:
            deleted_counts['empty_documents'] = empty_docs.delete()[0]
            self.stdout.write(f"  âœ… {deleted_counts['empty_documents']} boÅŸ dÃ¶kÃ¼man silindi")
        
        return {
            'cleanup_completed': True,
            'deleted_counts': deleted_counts,
            'cutoff_date': cutoff_date
        }
    
    def _rebuild_embeddings(self, options):
        """TÃ¼m embedding'leri yeniden oluÅŸtur"""
        chunks = KnowledgeChunk.objects.all()
        total_chunks = chunks.count()
        
        if options['dry_run']:
            self.stdout.write(f"  ğŸ“Š {total_chunks} chunk iÃ§in embedding yeniden oluÅŸturulacak")
            return {'total_chunks': total_chunks, 'dry_run': True}
        
        self.stdout.write(f"ğŸ§  {total_chunks} chunk iÃ§in embedding yeniden oluÅŸturuluyor...")
        updated_count = 0
        failed_count = 0
        batch_size = 50
       
        # Batch processing for better performance
        for i in range(0, total_chunks, batch_size):
            batch_chunks = chunks[i:i + batch_size]
            
            for chunk in batch_chunks:
                try:
                    # Generate new embedding
                    new_embedding = self.embedding_service.generate_embedding(chunk.content)
                    
                    # Update chunk
                    chunk.embedding = new_embedding
                    chunk.embedding_model = self.embedding_service.model_name
                    chunk.save(update_fields=['embedding', 'embedding_model'])
                    
                    updated_count += 1
                    
                    if options['verbose']:
                        self.stdout.write(f"  âœ… Updated: {chunk.source.title} - Chunk {chunk.id}")
                
                except Exception as e:
                    failed_count += 1
                    if options['verbose']:
                        self.stdout.write(f"  âŒ Failed: {chunk.id} - {str(e)}")
                    
                    if not options['force']:
                        break
            
            # Progress update
            progress = min((i + batch_size) / total_chunks * 100, 100)
            self.stdout.write(f"  Ä°lerleme: {progress:.1f}% ({updated_count + failed_count}/{total_chunks})")
        
        return {
            'total_chunks': total_chunks,
            'updated_count': updated_count,
            'failed_count': failed_count,
            'rebuild_completed': True
        }
    
    def _optimize_documents(self, options):
        """DÃ¶kÃ¼man optimizasyonu"""
        self.stdout.write("ğŸ”§ DÃ¶kÃ¼man optimizasyonu baÅŸlatÄ±lÄ±yor...")
        
        optimizations = {}
        
        # 1. Duplicate content removal
        duplicates = self._find_duplicate_chunks()
        if duplicates and not options['dry_run']:
            removed_duplicates = self._remove_duplicate_chunks(duplicates)
            optimizations['duplicates_removed'] = removed_duplicates
        else:
            optimizations['duplicates_found'] = len(duplicates)
        
        # 2. Low quality chunk removal
        low_quality = self._find_low_quality_chunks()
        if low_quality and not options['dry_run']:
            removed_low_quality = self._remove_low_quality_chunks(low_quality)
            optimizations['low_quality_removed'] = removed_low_quality
        else:
            optimizations['low_quality_found'] = len(low_quality)
        
        # 3. Update relevance scores
        if not options['dry_run']:
            updated_relevance = self._update_relevance_scores()
            optimizations['relevance_updated'] = updated_relevance
        
        # 4. Consolidate similar chunks
        if not options['dry_run']:
            consolidated = self._consolidate_similar_chunks()
            optimizations['chunks_consolidated'] = consolidated
        
        return {
            'optimization_completed': True,
            'optimizations': optimizations,
            'dry_run': options['dry_run']
        }
    
    def _find_duplicate_chunks(self):
        """Duplicate chunk'larÄ± bul"""
        from django.db.models import Count
        
        duplicates = KnowledgeChunk.objects.values('content_hash')\
            .annotate(count=Count('id'))\
            .filter(count__gt=1)
        
        duplicate_chunks = []
        for dup in duplicates:
            chunks = KnowledgeChunk.objects.filter(content_hash=dup['content_hash'])
            duplicate_chunks.extend(list(chunks[1:]))  # Ä°lkini tut, diÄŸerlerini sil
        
        return duplicate_chunks
    
    def _remove_duplicate_chunks(self, duplicates):
        """Duplicate chunk'larÄ± sil"""
        count = 0
        for chunk in duplicates:
            chunk.delete()
            count += 1
        return count
    
    def _find_low_quality_chunks(self):
        """DÃ¼ÅŸÃ¼k kaliteli chunk'larÄ± bul"""
        # Kriterler: Ã§ok kÄ±sa, Ã§ok az kelime, tekrarlayan iÃ§erik
        low_quality = KnowledgeChunk.objects.filter(
            models.Q(content_length__lt=50) |  # 50 karakterden kÄ±sa
            models.Q(content__icontains="...") |  # Eksik iÃ§erik
            models.Q(content__regex=r'^.{1,10}$')  # Ã‡ok kÄ±sa
        )
        
        return list(low_quality)
    
    def _remove_low_quality_chunks(self, low_quality_chunks):
        """DÃ¼ÅŸÃ¼k kaliteli chunk'larÄ± sil"""
        count = 0
        for chunk in low_quality_chunks:
            chunk.delete()
            count += 1
        return count
    
    def _update_relevance_scores(self):
        """Relevance score'larÄ± gÃ¼ncelle"""
        chunks = KnowledgeChunk.objects.all()
        updated_count = 0
        
        for chunk in chunks:
            # Usage count'a gÃ¶re relevance hesapla
            usage_score = min(chunk.usage_count / 10.0, 1.0)  # Max 1.0
            
            # Content quality'ye gÃ¶re score
            content_score = min(chunk.content_length / 1000.0, 1.0)  # Max 1.0
            
            # Combined score
            new_relevance = (usage_score * 0.6) + (content_score * 0.4)
            
            if abs(chunk.relevance_score - new_relevance) > 0.1:
                chunk.relevance_score = new_relevance
                chunk.save(update_fields=['relevance_score'])
                updated_count += 1
        
        return updated_count
    
    def _consolidate_similar_chunks(self):
        """Benzer chunk'larÄ± birleÅŸtir"""
        # Bu iÅŸlem Ã§ok karmaÅŸÄ±k olduÄŸu iÃ§in basit bir implementasyon
        # Gelecekte ML-based similarity detection eklenebilir
        consolidated_count = 0
        
        # Åimdilik sadece aynÄ± source ve page'deki kÃ¼Ã§Ã¼k chunk'larÄ± birleÅŸtir
        from django.db.models import Count
        
        sources_with_multiple_chunks = DocumentSource.objects.annotate(
            chunk_count=Count('chunks')
        ).filter(chunk_count__gt=5)
        
        for source in sources_with_multiple_chunks:
            pages = source.chunks.values('page_number').distinct()
            
            for page_data in pages:
                page_number = page_data['page_number']
                if page_number is None:
                    continue
                
                page_chunks = source.chunks.filter(
                    page_number=page_number,
                    content_length__lt=200
                ).order_by('created_at')
                
                if page_chunks.count() > 2:
                    # KÃ¼Ã§Ã¼k chunk'larÄ± birleÅŸtir
                    combined_content = '\n\n'.join([chunk.content for chunk in page_chunks])
                    
                    if len(combined_content) < 1500:  # Ã‡ok bÃ¼yÃ¼k olmasÄ±n
                        # Yeni birleÅŸtirilmiÅŸ chunk oluÅŸtur
                        first_chunk = page_chunks.first()
                        first_chunk.content = combined_content
                        first_chunk.content_length = len(combined_content)
                        
                        # Yeni embedding generate et
                        new_embedding = self.embedding_service.generate_embedding(combined_content)
                        first_chunk.embedding = new_embedding
                        first_chunk.save()
                        
                        # DiÄŸer chunk'larÄ± sil
                        page_chunks.exclude(id=first_chunk.id).delete()
                        consolidated_count += page_chunks.count() - 1
        
        return consolidated_count
    
    def _display_results(self, result, duration, options):
        """SonuÃ§larÄ± gÃ¶ster"""
        self.stdout.write("\n" + "="*60)
        self.stdout.write(self.style.SUCCESS("ğŸ“Š Ä°ÅŸlem TamamlandÄ±!"))
        self.stdout.write("="*60)
        
        # Duration
        duration_str = f"{duration:.1f} saniye"
        if duration > 60:
            duration_str = f"{duration/60:.1f} dakika"
        self.stdout.write(f"â±ï¸ SÃ¼re: {duration_str}")
        
        # Mode specific results
        if result.get('dry_run'):
            self.stdout.write(self.style.WARNING("\nğŸ” DRY RUN - HiÃ§bir deÄŸiÅŸiklik yapÄ±lmadÄ±"))
            
            if 'total_documents' in result:
                self.stdout.write(f"ğŸ“„ Analiz edilen dÃ¶kÃ¼man: {result['total_documents']:,}")
                if 'estimated_chunks' in result:
                    self.stdout.write(f"ğŸ§© Tahmini chunk: {result['estimated_chunks']:,}")
        
        elif result.get('cleanup_completed'):
            self.stdout.write(f"\nğŸ§¹ Cleanup TamamlandÄ±")
            for key, count in result.get('deleted_counts', {}).items():
                self.stdout.write(f"  â€¢ {key.replace('_', ' ').title()}: {count:,}")
        
        elif result.get('rebuild_completed'):
            self.stdout.write(f"\nğŸ§  Embedding Rebuild TamamlandÄ±")
            self.stdout.write(f"  â€¢ GÃ¼ncellenen: {result.get('updated_count', 0):,}")
            self.stdout.write(f"  â€¢ BaÅŸarÄ±sÄ±z: {result.get('failed_count', 0):,}")
        
        elif result.get('optimization_completed'):
            self.stdout.write(f"\nğŸ”§ Optimizasyon TamamlandÄ±")
            for key, count in result.get('optimizations', {}).items():
                self.stdout.write(f"  â€¢ {key.replace('_', ' ').title()}: {count:,}")
        
        elif result.get('mode') == 'async (Celery)':
            self.stdout.write(f"\nğŸ”„ Asenkron Ä°ÅŸlem BaÅŸlatÄ±ldÄ±")
            self.stdout.write(f"  â€¢ GÃ¶nderilen task: {result.get('tasks_submitted', 0):,}")
            self.stdout.write(f"  â€¢ Task ID'ler: {len(result.get('task_ids', []))}")
        
        else:
            # Normal processing results
            self.stdout.write(f"\nğŸ“Š Ä°ÅŸlem SonuÃ§larÄ± ({result.get('mode', 'unknown')})")
            
            if 'total_documents' in result:
                self.stdout.write(f"  â€¢ Toplam dÃ¶kÃ¼man: {result['total_documents']:,}")
            
            if 'processed' in result:
                self.stdout.write(f"  â€¢ Ä°ÅŸlenen: {result['processed']:,}")
                success_rate = (result.get('succeeded', 0) / result['processed']) * 100 if result['processed'] > 0 else 0
                self.stdout.write(f"  â€¢ BaÅŸarÄ±lÄ±: {result.get('succeeded', 0):,} ({success_rate:.1f}%)")
                self.stdout.write(f"  â€¢ BaÅŸarÄ±sÄ±z: {result.get('failed', 0):,}")
            
            if 'chunks_created' in result:
                self.stdout.write(f"  â€¢ OluÅŸturulan chunk: {result['chunks_created']:,}")
                if result['chunks_created'] > 0 and duration > 0:
                    chunks_per_sec = result['chunks_created'] / duration
                    self.stdout.write(f"  â€¢ Chunk/saniye: {chunks_per_sec:.1f}")
        
        # Errors
        if result.get('errors'):
            error_count = len(result['errors'])
            self.stdout.write(f"\nâŒ Hatalar ({error_count}):")
            
            # Ä°lk 5 hatayÄ± gÃ¶ster
            for i, error in enumerate(result['errors'][:5]):
                self.stdout.write(f"  {i+1}. {error['document']}: {error['error']}")
            
            if error_count > 5:
                self.stdout.write(f"  ... ve {error_count - 5} hata daha")
        
        # Performance stats
        if 'chunks_created' in result and result['chunks_created'] > 0:
            avg_time_per_chunk = duration / result['chunks_created']
            self.stdout.write(f"\nğŸ“ˆ Performans")
            self.stdout.write(f"  â€¢ Chunk baÅŸÄ±na ortalama sÃ¼re: {avg_time_per_chunk:.3f} saniye")
        
        # Recommendations
        self._display_recommendations(result, options)
    
    def _display_recommendations(self, result, options):
        """Ã–neriler gÃ¶ster"""
        recommendations = []
        
        # Performance recommendations
        if result.get('mode') == 'sequential' and result.get('total_documents', 0) > 10:
            recommendations.append("âš¡ BÃ¼yÃ¼k dÃ¶kÃ¼man setleri iÃ§in --batch modunu kullanÄ±n")
        
        if result.get('failed', 0) > 0 and not options.get('force'):
            recommendations.append("ğŸ”§ HatalarÄ± gÃ¶rmezden gelmek iÃ§in --force kullanÄ±n")
        
        # Optimization recommendations
        if result.get('chunks_created', 0) > 1000:
            recommendations.append("ğŸ”§ Ã‡ok sayÄ±da chunk oluÅŸtu, --optimize ile optimizasyon yapÄ±n")
        
        # Cleanup recommendations
        if not result.get('cleanup_completed') and not options.get('cleanup'):
            recommendations.append("ğŸ§¹ Periyodik temizlik iÃ§in --cleanup kullanÄ±n")
        
        # Async recommendations
        if result.get('total_documents', 0) > 50 and not options.get('async_mode'):
            recommendations.append("ğŸ”„ BÃ¼yÃ¼k iÅŸlemler iÃ§in --async-mode kullanÄ±n")
        
        if recommendations:
            self.stdout.write(f"\nğŸ’¡ Ã–neriler:")
            for rec in recommendations:
                self.stdout.write(f"  â€¢ {rec}")
        
        # Next steps
        self.stdout.write(f"\nğŸ“ Sonraki AdÄ±mlar:")
        
        if result.get('chunks_created', 0) > 0:
            self.stdout.write(f"  â€¢ Test sorgularÄ± ile sistem performansÄ±nÄ± kontrol edin")
            self.stdout.write(f"  â€¢ Analytics dashboard'u ile kullanÄ±m istatistiklerini inceleyin")
        
        if result.get('failed', 0) > 0:
            self.stdout.write(f"  â€¢ BaÅŸarÄ±sÄ±z dÃ¶kÃ¼manlarÄ±n loglarÄ±nÄ± kontrol edin")
            self.stdout.write(f"  â€¢ Problem olan dosyalarÄ± manuel olarak dÃ¼zeltin")
        
        if result.get('mode') == 'async (Celery)':
            self.stdout.write(f"  â€¢ Celery worker durumunu kontrol edin")
            self.stdout.write(f"  â€¢ Task sonuÃ§larÄ±nÄ± bekleyin ve kontrol edin")
        
        self.stdout.write("\n" + "="*60)
        self.stdout.write(self.style.SUCCESS("âœ¨ SAPBot Document Processing tamamlandÄ±!"))            
                        