# backend/sapbot_api/tasks/maintenance_tasks.py
"""
SAPBot API Maintenance Tasks

Bu mod√ºl sistem bakƒ±m g√∂revlerini i√ßerir:
- Cache temizleme
- Veritabanƒ± optimizasyonu
- Log rotasyonu
- Ge√ßici dosya temizleme
- Sistem saƒülƒ±k kontrol√º
- Performans optimizasyonu
"""

import os
import tempfile
from datetime import datetime, timedelta
from typing import models
from django.core.cache import cache
from django.db import transaction, connection
from django.utils import timezone
from django.conf import settings
from celery import shared_task
from celery.utils.log import get_task_logger

from ..models import (
   DocumentSource, KnowledgeChunk, ChatConversation, ChatMessage,
   UserSession, SystemLog, ErrorLog, QueryAnalytics, SystemMetrics,
   PerformanceMetrics
)
from ..utils.cache_utils import clear_all_cache, get_cache_stats
from ..utils.helpers import format_file_size, time_ago

logger = get_task_logger(__name__)


@shared_task(bind=True, max_retries=3, default_retry_delay=300)
def cleanup_expired_sessions(self):
   """S√ºresi dolmu≈ü oturumlarƒ± temizle"""
   try:
       logger.info("üßπ S√ºresi dolmu≈ü oturumlar temizleniyor...")
       
       # S√ºresi dolmu≈ü oturumlarƒ± bul
       expired_sessions = UserSession.objects.filter(
           expires_at__lt=timezone.now()
       )
       
       expired_count = expired_sessions.count()
       
       if expired_count > 0:
           # Toplu silme
           expired_sessions.delete()
           logger.info(f"‚úÖ {expired_count} s√ºresi dolmu≈ü oturum silindi")
       else:
           logger.info("‚ÑπÔ∏è Silinecek s√ºresi dolmu≈ü oturum bulunamadƒ±")
       
       # Cache'den de temizle
       cache.delete_pattern("user_session:*")
       
       return {
           'success': True,
           'expired_sessions_deleted': expired_count,
           'timestamp': timezone.now().isoformat()
       }
       
   except Exception as exc:
       logger.error(f"‚ùå Oturum temizleme hatasƒ±: {exc}")
       raise self.retry(exc=exc)


@shared_task(bind=True, max_retries=2, default_retry_delay=600)
def cleanup_old_conversations(self, days_old: int = 30):
   """Eski konu≈ümalarƒ± temizle"""
   try:
       logger.info(f"üßπ {days_old} g√ºnden eski konu≈ümalar temizleniyor...")
       
       cutoff_date = timezone.now() - timedelta(days=days_old)
       
       # Eski konu≈ümalarƒ± bul
       old_conversations = ChatConversation.objects.filter(
           last_activity__lt=cutoff_date,
           is_archived=False
       )
       
       conversation_count = old_conversations.count()
       
       if conversation_count > 0:
           # √ñnce ar≈üivle, sonra sil
           with transaction.atomic():
               # Ar≈üivle
               old_conversations.update(is_archived=True)
               
               # Mesajlarƒ± da sil
               old_messages = ChatMessage.objects.filter(
                   conversation__in=old_conversations
               )
               message_count = old_messages.count()
               old_messages.delete()
               
               # Konu≈ümalarƒ± sil
               old_conversations.delete()
               
           logger.info(f"‚úÖ {conversation_count} konu≈üma ve {message_count} mesaj silindi")
       else:
           logger.info("‚ÑπÔ∏è Silinecek eski konu≈üma bulunamadƒ±")
       
       return {
           'success': True,
           'conversations_deleted': conversation_count,
           'cutoff_date': cutoff_date.isoformat(),
           'timestamp': timezone.now().isoformat()
       }
       
   except Exception as exc:
       logger.error(f"‚ùå Konu≈üma temizleme hatasƒ±: {exc}")
       raise self.retry(exc=exc)


@shared_task(bind=True, max_retries=2)
def cleanup_old_logs(self, days_old: int = 7):
   """Eski log kayƒ±tlarƒ±nƒ± temizle"""
   try:
       logger.info(f"üßπ {days_old} g√ºnden eski loglar temizleniyor...")
       
       cutoff_date = timezone.now() - timedelta(days=days_old)
       
       # System logs
       old_system_logs = SystemLog.objects.filter(created_at__lt=cutoff_date)
       system_log_count = old_system_logs.count()
       old_system_logs.delete()
       
       # Error logs (biraz daha uzun tut)
       error_cutoff = timezone.now() - timedelta(days=days_old * 2)
       old_error_logs = ErrorLog.objects.filter(
           created_at__lt=error_cutoff,
           is_resolved=True
       )
       error_log_count = old_error_logs.count()
       old_error_logs.delete()
       
       # Performance metrics
       old_metrics = PerformanceMetrics.objects.filter(
           timestamp__lt=cutoff_date
       )
       metrics_count = old_metrics.count()
       old_metrics.delete()
       
       logger.info(f"‚úÖ {system_log_count} sistem log, {error_log_count} hata log, {metrics_count} metrik silindi")
       
       return {
           'success': True,
           'system_logs_deleted': system_log_count,
           'error_logs_deleted': error_log_count,
           'metrics_deleted': metrics_count,
           'timestamp': timezone.now().isoformat()
       }
       
   except Exception as exc:
       logger.error(f"‚ùå Log temizleme hatasƒ±: {exc}")
       raise self.retry(exc=exc)


@shared_task(bind=True, max_retries=2)
def optimize_database(self):
   """Veritabanƒ± optimizasyonu"""
   try:
       logger.info("üîß Veritabanƒ± optimizasyonu ba≈ülƒ±yor...")
       
       optimization_results = {}
       
       with connection.cursor() as cursor:
           # PostgreSQL i√ßin vacuum ve analyze
           tables_to_optimize = [
               'sapbot_document_sources',
               'sapbot_knowledge_chunks',
               'sapbot_chat_conversations',
               'sapbot_chat_messages',
               'sapbot_query_analytics',
               'sapbot_user_sessions'
           ]
           
           for table in tables_to_optimize:
               try:
                   # VACUUM ANALYZE
                   cursor.execute(f"VACUUM ANALYZE {table}")
                   logger.info(f"‚úÖ {table} optimize edildi")
                   optimization_results[table] = 'optimized'
               except Exception as e:
                   logger.warning(f"‚ö†Ô∏è {table} optimize edilemedi: {e}")
                   optimization_results[table] = f'failed: {str(e)}'
           
           # ƒ∞ndeks kullanƒ±m istatistikleri
           cursor.execute("""
               SELECT schemaname, tablename, indexname, idx_tup_read, idx_tup_fetch
               FROM pg_stat_user_indexes 
               WHERE schemaname = 'public' 
               AND tablename LIKE 'sapbot_%'
               ORDER BY idx_tup_read DESC
               LIMIT 10
           """)
           
           index_stats = cursor.fetchall()
           optimization_results['top_used_indexes'] = [
               {
                   'table': row[1],
                   'index': row[2], 
                   'reads': row[3],
                   'fetches': row[4]
               } for row in index_stats
           ]
           
           # Tablo boyutlarƒ±
           cursor.execute("""
               SELECT 
                   tablename,
                   pg_size_pretty(pg_total_relation_size(tablename::regclass)) as size
               FROM pg_tables 
               WHERE schemaname = 'public' 
               AND tablename LIKE 'sapbot_%'
               ORDER BY pg_total_relation_size(tablename::regclass) DESC
           """)
           
           table_sizes = cursor.fetchall()
           optimization_results['table_sizes'] = [
               {'table': row[0], 'size': row[1]} for row in table_sizes
           ]
       
       logger.info("‚úÖ Veritabanƒ± optimizasyonu tamamlandƒ±")
       
       return {
           'success': True,
           'optimization_results': optimization_results,
           'timestamp': timezone.now().isoformat()
       }
       
   except Exception as exc:
       logger.error(f"‚ùå Veritabanƒ± optimizasyon hatasƒ±: {exc}")
       raise self.retry(exc=exc)


@shared_task(bind=True, max_retries=2)
def cleanup_temp_files(self):
   """Ge√ßici dosyalarƒ± temizle"""
   try:
       logger.info("üßπ Ge√ßici dosyalar temizleniyor...")
       
       temp_dirs = [
           getattr(settings, 'TEMP_DIR', tempfile.gettempdir()),
           os.path.join(settings.MEDIA_ROOT, 'temp'),
           '/tmp'
       ]
       
       total_files_deleted = 0
       total_size_freed = 0
       
       for temp_dir in temp_dirs:
           if not os.path.exists(temp_dir):
               continue
               
           logger.info(f"üîç {temp_dir} dizini kontrol ediliyor...")
           
           for filename in os.listdir(temp_dir):
               file_path = os.path.join(temp_dir, filename)
               
               if not os.path.isfile(file_path):
                   continue
               
               # 1 saatten eski ge√ßici dosyalarƒ± sil
               file_age = datetime.now().timestamp() - os.path.getctime(file_path)
               
               if file_age > 3600:  # 1 saat
                   try:
                       file_size = os.path.getsize(file_path)
                       os.unlink(file_path)
                       total_files_deleted += 1
                       total_size_freed += file_size
                       logger.debug(f"üóëÔ∏è Silindi: {filename}")
                   except OSError as e:
                       logger.warning(f"‚ö†Ô∏è Dosya silinemiyor {filename}: {e}")
       
       logger.info(f"‚úÖ {total_files_deleted} dosya silindi, {format_file_size(total_size_freed)} yer bo≈üaltƒ±ldƒ±")
       
       return {
           'success': True,
           'files_deleted': total_files_deleted,
           'size_freed': total_size_freed,
           'size_freed_formatted': format_file_size(total_size_freed),
           'timestamp': timezone.now().isoformat()
       }
       
   except Exception as exc:
       logger.error(f"‚ùå Ge√ßici dosya temizleme hatasƒ±: {exc}")
       raise self.retry(exc=exc)


@shared_task(bind=True, max_retries=2)
def cleanup_cache(self):
   """Cache temizleme"""
   try:
       logger.info("üßπ Cache temizleniyor...")
       
       # Cache istatistiklerini al
       cache_stats_before = get_cache_stats()
       
       # T√ºm cache'i temizle
       clear_result = clear_all_cache()
       
       # Cache istatistiklerini tekrar al
       cache_stats_after = get_cache_stats()
       
       if clear_result:
           logger.info("‚úÖ Cache ba≈üarƒ±yla temizlendi")
       else:
           logger.warning("‚ö†Ô∏è Cache temizleme sƒ±rasƒ±nda sorun olu≈ütu")
       
       return {
           'success': clear_result,
           'cache_stats_before': cache_stats_before,
           'cache_stats_after': cache_stats_after,
           'timestamp': timezone.now().isoformat()
       }
       
   except Exception as exc:
       logger.error(f"‚ùå Cache temizleme hatasƒ±: {exc}")
       raise self.retry(exc=exc)


@shared_task(bind=True, max_retries=3)
def system_health_check(self):
   """Sistem saƒülƒ±k kontrol√º"""
   try:
       logger.info("üè• Sistem saƒülƒ±k kontrol√º ba≈ülƒ±yor...")
       
       health_status = {
           'overall_status': 'healthy',
           'checks': {},
           'recommendations': [],
           'timestamp': timezone.now().isoformat()
       }
       
       # Veritabanƒ± kontrol√º
       try:
           with connection.cursor() as cursor:
               cursor.execute("SELECT 1")
               result = cursor.fetchone()
               if result[0] == 1:
                   health_status['checks']['database'] = 'healthy'
               else:
                   health_status['checks']['database'] = 'warning'
       except Exception as e:
           health_status['checks']['database'] = 'critical'
           health_status['overall_status'] = 'critical'
           logger.error(f"‚ùå Veritabanƒ± saƒülƒ±k kontrol√º ba≈üarƒ±sƒ±z: {e}")
       
       # Cache kontrol√º
       try:
           cache.set('health_check', 'test', 60)
           if cache.get('health_check') == 'test':
               health_status['checks']['cache'] = 'healthy'
               cache.delete('health_check')
           else:
               health_status['checks']['cache'] = 'warning'
       except Exception as e:
           health_status['checks']['cache'] = 'critical'
           logger.error(f"‚ùå Cache saƒülƒ±k kontrol√º ba≈üarƒ±sƒ±z: {e}")
       
       # Disk alanƒ± kontrol√º
       try:
           disk_usage = os.statvfs('/')
           free_space = disk_usage.f_bavail * disk_usage.f_frsize
           total_space = disk_usage.f_blocks * disk_usage.f_frsize
           usage_percent = ((total_space - free_space) / total_space) * 100
           
           if usage_percent > 90:
               health_status['checks']['disk_space'] = 'critical'
               health_status['recommendations'].append(
                   f"Disk kullanƒ±mƒ± %{usage_percent:.1f} - Acil disk temizliƒüi gerekli"
               )
               if health_status['overall_status'] == 'healthy':
                   health_status['overall_status'] = 'critical'
           elif usage_percent > 80:
               health_status['checks']['disk_space'] = 'warning'
               health_status['recommendations'].append(
                   f"Disk kullanƒ±mƒ± %{usage_percent:.1f} - Disk temizliƒüi √∂nerilir"
               )
               if health_status['overall_status'] == 'healthy':
                   health_status['overall_status'] = 'warning'
           else:
               health_status['checks']['disk_space'] = 'healthy'
               
           health_status['disk_usage_percent'] = round(usage_percent, 1)
           health_status['free_space'] = format_file_size(free_space)
           
       except Exception as e:
           health_status['checks']['disk_space'] = 'unknown'
           logger.error(f"‚ùå Disk alanƒ± kontrol√º ba≈üarƒ±sƒ±z: {e}")
       
       # Veritabanƒ± baƒülantƒ± sayƒ±sƒ±
       try:
           with connection.cursor() as cursor:
               cursor.execute("SELECT count(*) FROM pg_stat_activity")
               connection_count = cursor.fetchone()[0]
               
               max_connections = 100  # PostgreSQL default
               connection_usage = (connection_count / max_connections) * 100
               
               if connection_usage > 80:
                   health_status['checks']['db_connections'] = 'warning'
                   health_status['recommendations'].append(
                       f"Veritabanƒ± baƒülantƒ± kullanƒ±mƒ± %{connection_usage:.1f}"
                   )
               else:
                   health_status['checks']['db_connections'] = 'healthy'
                   
               health_status['db_connections'] = {
                   'current': connection_count,
                   'usage_percent': round(connection_usage, 1)
               }
       except Exception as e:
           health_status['checks']['db_connections'] = 'unknown'
           logger.error(f"‚ùå DB baƒülantƒ± kontrol√º ba≈üarƒ±sƒ±z: {e}")
       
       # Bellek kullanƒ±mƒ± (basit kontrol)
       try:
           import psutil
           memory = psutil.virtual_memory()
           
           if memory.percent > 90:
               health_status['checks']['memory'] = 'critical'
               health_status['recommendations'].append(
                   f"Bellek kullanƒ±mƒ± %{memory.percent:.1f} - Y√ºksek bellek kullanƒ±mƒ±"
               )
           elif memory.percent > 80:
               health_status['checks']['memory'] = 'warning'
           else:
               health_status['checks']['memory'] = 'healthy'
               
           health_status['memory_usage_percent'] = round(memory.percent, 1)
           
       except ImportError:
           health_status['checks']['memory'] = 'unknown'
           logger.warning("‚ö†Ô∏è psutil y√ºkl√º deƒüil, bellek kontrol√º yapƒ±lamƒ±yor")
       except Exception as e:
           health_status['checks']['memory'] = 'unknown'
           logger.error(f"‚ùå Bellek kontrol√º ba≈üarƒ±sƒ±z: {e}")
       
       # Model sayƒ±larƒ±
       try:
           health_status['model_counts'] = {
               'documents': DocumentSource.objects.count(),
               'chunks': KnowledgeChunk.objects.count(),
               'conversations': ChatConversation.objects.count(),
               'messages': ChatMessage.objects.count(),
               'active_sessions': UserSession.objects.filter(
                   expires_at__gt=timezone.now()
               ).count()
           }
       except Exception as e:
           logger.error(f"‚ùå Model sayƒ±m hatasƒ±: {e}")
       
       # Genel durum deƒüerlendirme
       critical_checks = [k for k, v in health_status['checks'].items() if v == 'critical']
       warning_checks = [k for k, v in health_status['checks'].items() if v == 'warning']
       
       if critical_checks:
           health_status['overall_status'] = 'critical'
       elif warning_checks:
           health_status['overall_status'] = 'warning'
       
       # Sistem metriƒüi kaydet
       SystemMetrics.objects.create(
           metric_name='system_health_check',
           metric_type='gauge',
           value=1 if health_status['overall_status'] == 'healthy' else 0,
           labels={
               'status': health_status['overall_status'],
               'critical_checks': len(critical_checks),
               'warning_checks': len(warning_checks)
           }
       )
       
       logger.info(f"‚úÖ Sistem saƒülƒ±k kontrol√º tamamlandƒ± - Durum: {health_status['overall_status']}")
       
       return health_status
       
   except Exception as exc:
       logger.error(f"‚ùå Sistem saƒülƒ±k kontrol√º hatasƒ±: {exc}")
       raise self.retry(exc=exc)


@shared_task(bind=True, max_retries=2)
def generate_analytics_summary(self, period: str = 'daily'):
   """Analitik √∂zet olu≈ütur"""
   try:
       logger.info(f"üìä {period} analitik √∂zet olu≈üturuluyor...")
       
       now = timezone.now()
       
       if period == 'daily':
           start_date = now.replace(hour=0, minute=0, second=0, microsecond=0)
           end_date = start_date + timedelta(days=1)
       elif period == 'weekly':
           start_date = now - timedelta(days=now.weekday())
           start_date = start_date.replace(hour=0, minute=0, second=0, microsecond=0)
           end_date = start_date + timedelta(days=7)
       elif period == 'monthly':
           start_date = now.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
           if now.month == 12:
               end_date = start_date.replace(year=now.year + 1, month=1)
           else:
               end_date = start_date.replace(month=now.month + 1)
       else:
           raise ValueError(f"Ge√ßersiz period: {period}")
       
       # Analitik verilerini topla
       analytics = QueryAnalytics.objects.filter(
           created_at__gte=start_date,
           created_at__lt=end_date
       )
       
       summary = {
           'period': period,
           'start_date': start_date.isoformat(),
           'end_date': end_date.isoformat(),
           'total_queries': analytics.count(),
           'successful_queries': analytics.filter(response_generated=True).count(),
           'unique_users': analytics.values('user').distinct().count(),
           'unique_sessions': analytics.values('session_id').distinct().count(),
           'avg_response_time': 0,
           'top_sap_modules': {},
           'top_intents': {},
           'error_breakdown': {},
           'timestamp': timezone.now().isoformat()
       }
       
       # Ortalama yanƒ±t s√ºresi
       response_times = analytics.filter(
           response_time__isnull=False
       ).values_list('response_time', flat=True)
       
       if response_times:
           summary['avg_response_time'] = sum(response_times) / len(response_times)
       
       # En √ßok kullanƒ±lan SAP mod√ºlleri
       sap_modules = analytics.filter(
           sap_module_detected__isnull=False
       ).values_list('sap_module_detected', flat=True)
       
       from collections import Counter
       summary['top_sap_modules'] = dict(Counter(sap_modules).most_common(5))
       
       # En √ßok kullanƒ±lan niyetler
       intents = analytics.filter(
           intent_detected__isnull=False
       ).values_list('intent_detected', flat=True)
       
       summary['top_intents'] = dict(Counter(intents).most_common(5))
       
       # Hata daƒüƒ±lƒ±mƒ±
       errors = analytics.filter(
           error_occurred=True
       ).values_list('error_type', flat=True)
       
       summary['error_breakdown'] = dict(Counter(errors).most_common(5))
       
       # Ba≈üarƒ± oranƒ±
       if summary['total_queries'] > 0:
           summary['success_rate'] = (
               summary['successful_queries'] / summary['total_queries']
           ) * 100
       else:
           summary['success_rate'] = 0
       
       logger.info(f"‚úÖ {period} analitik √∂zet olu≈üturuldu - {summary['total_queries']} sorgu")
       
       return summary
       
   except Exception as exc:
       logger.error(f"‚ùå Analitik √∂zet olu≈üturma hatasƒ±: {exc}")
       raise self.retry(exc=exc)


@shared_task(bind=True, max_retries=2)
def backup_critical_data(self):
   """Kritik verilerin yedeklenmesi"""
   try:
       logger.info("üíæ Kritik veri yedekleme ba≈ülƒ±yor...")
       
       backup_results = {
           'success': False,
           'backup_files': [],
           'timestamp': timezone.now().isoformat()
       }
       
       # Backup dizinini olu≈ütur
       backup_dir = os.path.join(settings.MEDIA_ROOT, 'backups')
       os.makedirs(backup_dir, exist_ok=True)
       
       timestamp = timezone.now().strftime('%Y%m%d_%H%M%S')
       
       # Sistem konfig√ºrasyonu yedekle
       from ..models import SystemConfiguration
       configs = SystemConfiguration.objects.all().values()
       
       config_backup_file = os.path.join(backup_dir, f'system_config_{timestamp}.json')
       
       import json
       with open(config_backup_file, 'w', encoding='utf-8') as f:
           json.dump(list(configs), f, ensure_ascii=False, indent=2, default=str)
       
       backup_results['backup_files'].append({
           'type': 'system_config',
           'file': config_backup_file,
           'size': os.path.getsize(config_backup_file),
           'records': len(configs)
       })
       
       # Kullanƒ±cƒ± profilleri yedekle
       from ..models import UserProfile
       profiles = UserProfile.objects.all().values()
       
       profiles_backup_file = os.path.join(backup_dir, f'user_profiles_{timestamp}.json')
       
       with open(profiles_backup_file, 'w', encoding='utf-8') as f:
           json.dump(list(profiles), f, ensure_ascii=False, indent=2, default=str)
       
       backup_results['backup_files'].append({
           'type': 'user_profiles',
           'file': profiles_backup_file,
           'size': os.path.getsize(profiles_backup_file),
           'records': len(profiles)
       })
       
       # Eski yedekleri temizle (30 g√ºn)
       old_backups = []
       cutoff_time = timezone.now() - timedelta(days=30)
       
       for filename in os.listdir(backup_dir):
           file_path = os.path.join(backup_dir, filename)
           if os.path.isfile(file_path):
               file_time = datetime.fromtimestamp(os.path.getctime(file_path))
               if file_time < cutoff_time.replace(tzinfo=None):
                   try:
                       os.unlink(file_path)
                       old_backups.append(filename)
                   except OSError:
                       pass
       
       backup_results['old_backups_deleted'] = old_backups
       backup_results['success'] = True
       
       logger.info(f"‚úÖ Kritik veri yedekleme tamamlandƒ± - {len(backup_results['backup_files'])} dosya")
       
       return backup_results
       
   except Exception as exc:
       logger.error(f"‚ùå Veri yedekleme hatasƒ±: {exc}")
       raise self.retry(exc=exc)


@shared_task(bind=True)
def daily_maintenance():
   """G√ºnl√ºk bakƒ±m g√∂revi"""
   try:
       logger.info("üîß G√ºnl√ºk bakƒ±m ba≈ülƒ±yor...")
       
       maintenance_results = {
           'tasks_completed': [],
           'tasks_failed': [],
           'start_time': timezone.now().isoformat()
       }
       
       # G√∂revleri sƒ±rayla √ßalƒ±≈ütƒ±r
       maintenance_tasks = [
           ('cleanup_expired_sessions', cleanup_expired_sessions),
           ('cleanup_temp_files', cleanup_temp_files),
           ('cleanup_cache', cleanup_cache),
           ('system_health_check', system_health_check),
           ('generate_analytics_summary', lambda: generate_analytics_summary.delay('daily'))
       ]
       
       for task_name, task_func in maintenance_tasks:
           try:
               if callable(task_func):
                   result = task_func()
                   maintenance_results['tasks_completed'].append({
                       'task': task_name,
                       'result': result if isinstance(result, dict) else str(result),
                       'completed_at': timezone.now().isoformat()
                   })
                   logger.info(f"‚úÖ {task_name} tamamlandƒ±")
               else:
                   # Celery task
                   task_func
                   maintenance_results['tasks_completed'].append({
                       'task': task_name,
                       'status': 'scheduled',
                       'completed_at': timezone.now().isoformat()
                   })
                   logger.info(f"üìÖ {task_name} planlandƒ±")
                   
           except Exception as e:
               maintenance_results['tasks_failed'].append({
                   'task': task_name,
                   'error': str(e),
                   'failed_at': timezone.now().isoformat()
               })
               logger.error(f"‚ùå {task_name} ba≈üarƒ±sƒ±z: {e}")
       
       maintenance_results['end_time'] = timezone.now().isoformat()
       
       # Bakƒ±m metriƒüi kaydet
       SystemMetrics.objects.create(
           metric_name='daily_maintenance',
           metric_type='counter',
           value=1,
           labels={
               'completed_tasks': len(maintenance_results['tasks_completed']),
               'failed_tasks': len(maintenance_results['tasks_failed'])
           }
       )
       
       logger.info("‚úÖ G√ºnl√ºk bakƒ±m tamamlandƒ±")
       
       return maintenance_results
       
   except Exception as exc:
       logger.error(f"‚ùå G√ºnl√ºk bakƒ±m hatasƒ±: {exc}")
       return {
           'success': False,
           'error': str(exc),
           'timestamp': timezone.now().isoformat()
       }


@shared_task(bind=True)
def weekly_maintenance():
   """Haftalƒ±k bakƒ±m g√∂revi"""
   try:
       logger.info("üîß Haftalƒ±k bakƒ±m ba≈ülƒ±yor...")
       
       maintenance_results = {
           'tasks_completed': [],
           'tasks_failed': [],
           'start_time': timezone.now().isoformat()
       }
       
       # Haftalƒ±k g√∂revler
       weekly_tasks = [
           ('cleanup_old_conversations', lambda: cleanup_old_conversations.delay(30)),
           ('optimize_database', optimize_database),
           ('backup_critical_data', backup_critical_data),
           ('generate_weekly_analytics', lambda: generate_analytics_summary.delay('weekly'))
       ]
       
       for task_name, task_func in weekly_tasks:
           try:
               if callable(task_func):
                   result = task_func()
                   maintenance_results['tasks_completed'].append({
                       'task': task_name,
                       'result': result if isinstance(result, dict) else str(result),
                       'completed_at': timezone.now().isoformat()
                   })
                   logger.info(f"‚úÖ {task_name} tamamlandƒ±")
               else:
                   task_func
                   maintenance_results['tasks_completed'].append({
                       'task': task_name,
                       'status': 'scheduled',
                       'completed_at': timezone.now().isoformat()
                   })
                   
           except Exception as e:
               maintenance_results['tasks_failed'].append({
                   'task': task_name,
                   'error': str(e),
                   'failed_at': timezone.now().isoformat()
               })
               logger.error(f"‚ùå {task_name} ba≈üarƒ±sƒ±z: {e}")
       
       maintenance_results['end_time'] = timezone.now().isoformat()
       
       # Haftalƒ±k bakƒ±m metriƒüi
       SystemMetrics.objects.create(
           metric_name='weekly_maintenance',
           metric_type='counter',
           value=1,
           labels={
               'completed_tasks': len(maintenance_results['tasks_completed']),
               'failed_tasks': len(maintenance_results['tasks_failed'])
           }
       )
       
       logger.info("‚úÖ Haftalƒ±k bakƒ±m tamamlandƒ±")
       
       return maintenance_results
       
   except Exception as exc:
       logger.error(f"‚ùå Haftalƒ±k bakƒ±m hatasƒ±: {exc}")
       return {
           'success': False,
           'error': str(exc),
           'timestamp': timezone.now().isoformat()
       }


@shared_task(bind=True, max_retries=1)
def emergency_cleanup(self):
   """Acil durum temizliƒüi - disk dolduƒüunda"""
   try:
       logger.warning("üö® ACƒ∞L DURUM TEMƒ∞ZLƒ∞ƒûƒ∞ BA≈ûLIYOR!")
       
       cleanup_results = {
           'actions_taken': [],
           'space_freed': 0,
           'timestamp': timezone.now().isoformat()
       }
       
       # 1. T√ºm cache'i temizle
       try:
           clear_all_cache()
           cleanup_results['actions_taken'].append('cache_cleared')
           logger.info("‚úÖ Cache tamamen temizlendi")
       except Exception as e:
           logger.error(f"‚ùå Cache temizleme hatasƒ±: {e}")
       
       # 2. Ge√ßici dosyalarƒ± agresif temizle
       try:
           temp_result = cleanup_temp_files.delay()
           cleanup_results['actions_taken'].append('temp_files_cleaned')
           logger.info("‚úÖ Ge√ßici dosyalar temizlendi")
       except Exception as e:
           logger.error(f"‚ùå Ge√ßici dosya temizleme hatasƒ±: {e}")
       
       # 3. Eski loglarƒ± agresif temizle (1 g√ºnl√ºk)
       try:
           cutoff_date = timezone.now() - timedelta(days=1)
           
           # System logs
           old_logs = SystemLog.objects.filter(created_at__lt=cutoff_date)
           log_count = old_logs.count()
           old_logs.delete()
           
           # Performance metrics
           old_metrics = PerformanceMetrics.objects.filter(timestamp__lt=cutoff_date)
           metrics_count = old_metrics.count()
           old_metrics.delete()
           
           cleanup_results['actions_taken'].append(f'deleted_{log_count}_logs_{metrics_count}_metrics')
           logger.info(f"‚úÖ {log_count} log ve {metrics_count} metrik silindi")
           
       except Exception as e:
           logger.error(f"‚ùå Log temizleme hatasƒ±: {e}")
       
       # 4. Eski konu≈ümalarƒ± agresif temizle (7 g√ºnl√ºk)
       try:
           cutoff_date = timezone.now() - timedelta(days=7)
           old_conversations = ChatConversation.objects.filter(
               last_activity__lt=cutoff_date
           )
           conversation_count = old_conversations.count()
           
           if conversation_count > 0:
               old_messages = ChatMessage.objects.filter(
                   conversation__in=old_conversations
               )
               message_count = old_messages.count()
               
               with transaction.atomic():
                   old_messages.delete()
                   old_conversations.delete()
               
               cleanup_results['actions_taken'].append(
                   f'deleted_{conversation_count}_conversations_{message_count}_messages'
               )
               logger.info(f"‚úÖ {conversation_count} konu≈üma ve {message_count} mesaj silindi")
               
       except Exception as e:
           logger.error(f"‚ùå Konu≈üma temizleme hatasƒ±: {e}")
       
       # 5. Veritabanƒ± vacuum
       try:
           with connection.cursor() as cursor:
               cursor.execute("VACUUM FULL")
           cleanup_results['actions_taken'].append('database_vacuum_full')
           logger.info("‚úÖ Veritabanƒ± vacuum full tamamlandƒ±")
       except Exception as e:
           logger.error(f"‚ùå Database vacuum hatasƒ±: {e}")
       
       # 6. Disk kullanƒ±mƒ±nƒ± tekrar kontrol et
       try:
           disk_usage = os.statvfs('/')
           free_space = disk_usage.f_bavail * disk_usage.f_frsize
           total_space = disk_usage.f_blocks * disk_usage.f_frsize
           usage_percent = ((total_space - free_space) / total_space) * 100
           
           cleanup_results['final_disk_usage'] = round(usage_percent, 1)
           cleanup_results['free_space'] = format_file_size(free_space)
           
       except Exception as e:
           logger.error(f"‚ùå Disk kontrol√º hatasƒ±: {e}")
       
       logger.warning("üö® ACƒ∞L DURUM TEMƒ∞ZLƒ∞ƒûƒ∞ TAMAMLANDI!")
       
       # Acil durum metriƒüi
       SystemMetrics.objects.create(
           metric_name='emergency_cleanup',
           metric_type='counter',
           value=1,
           labels={
               'actions_count': len(cleanup_results['actions_taken']),
               'trigger': 'disk_full'
           }
       )
       
       return cleanup_results
       
   except Exception as exc:
       logger.error(f"‚ùå Acil durum temizliƒüi hatasƒ±: {exc}")
       raise self.retry(exc=exc)


@shared_task(bind=True, max_retries=2)
def monitor_system_resources(self):
   """Sistem kaynaklarƒ±nƒ± izle"""
   try:
       logger.info("üìä Sistem kaynaklarƒ± izleniyor...")
       
       monitoring_results = {
           'status': 'healthy',
           'alerts': [],
           'metrics': {},
           'timestamp': timezone.now().isoformat()
       }
       
       # Disk kullanƒ±mƒ±
       try:
           disk_usage = os.statvfs('/')
           free_space = disk_usage.f_bavail * disk_usage.f_frsize
           total_space = disk_usage.f_blocks * disk_usage.f_frsize
           usage_percent = ((total_space - free_space) / total_space) * 100
           
           monitoring_results['metrics']['disk_usage_percent'] = round(usage_percent, 1)
           monitoring_results['metrics']['free_space_gb'] = round(free_space / (1024**3), 2)
           
           # Disk uyarƒ±larƒ±
           if usage_percent > 95:
               monitoring_results['alerts'].append({
                   'level': 'critical',
                   'message': f'Disk kullanƒ±mƒ± kritik seviyede: %{usage_percent:.1f}',
                   'action': 'emergency_cleanup_needed'
               })
               monitoring_results['status'] = 'critical'
               
               # Acil temizleme ba≈ülat
               emergency_cleanup.delay()
               
           elif usage_percent > 85:
               monitoring_results['alerts'].append({
                   'level': 'warning',
                   'message': f'Disk kullanƒ±mƒ± y√ºksek: %{usage_percent:.1f}',
                   'action': 'cleanup_recommended'
               })
               if monitoring_results['status'] == 'healthy':
                   monitoring_results['status'] = 'warning'
           
           # Disk metriƒüi kaydet
           SystemMetrics.objects.create(
               metric_name='disk_usage_percent',
               metric_type='gauge',
               value=usage_percent
           )
           
       except Exception as e:
           logger.error(f"‚ùå Disk izleme hatasƒ±: {e}")
       
       # Veritabanƒ± baƒülantƒ± sayƒ±sƒ±
       try:
           with connection.cursor() as cursor:
               cursor.execute("SELECT count(*) FROM pg_stat_activity WHERE state = 'active'")
               active_connections = cursor.fetchone()[0]
               
               cursor.execute("SELECT setting FROM pg_settings WHERE name = 'max_connections'")
               max_connections = int(cursor.fetchone()[0])
               
               connection_usage = (active_connections / max_connections) * 100
               
               monitoring_results['metrics']['db_connections'] = {
                   'active': active_connections,
                   'max': max_connections,
                   'usage_percent': round(connection_usage, 1)
               }
               
               if connection_usage > 80:
                   monitoring_results['alerts'].append({
                       'level': 'warning',
                       'message': f'Veritabanƒ± baƒülantƒ± kullanƒ±mƒ± y√ºksek: %{connection_usage:.1f}',
                       'action': 'connection_pool_optimization_needed'
                   })
                   if monitoring_results['status'] == 'healthy':
                       monitoring_results['status'] = 'warning'
               
               # DB connection metriƒüi
               SystemMetrics.objects.create(
                   metric_name='db_active_connections',
                   metric_type='gauge',
                   value=active_connections
               )
               
       except Exception as e:
           logger.error(f"‚ùå DB baƒülantƒ± izleme hatasƒ±: {e}")
       
       # Bellek kullanƒ±mƒ± (psutil varsa)
       try:
           import psutil
           memory = psutil.virtual_memory()
           
           monitoring_results['metrics']['memory_usage_percent'] = round(memory.percent, 1)
           monitoring_results['metrics']['memory_available_gb'] = round(memory.available / (1024**3), 2)
           
           if memory.percent > 90:
               monitoring_results['alerts'].append({
                   'level': 'critical',
                   'message': f'Bellek kullanƒ±mƒ± kritik: %{memory.percent:.1f}',
                   'action': 'restart_required'
               })
               monitoring_results['status'] = 'critical'
           elif memory.percent > 80:
               monitoring_results['alerts'].append({
                   'level': 'warning',
                   'message': f'Bellek kullanƒ±mƒ± y√ºksek: %{memory.percent:.1f}',
                   'action': 'memory_optimization_needed'
               })
               if monitoring_results['status'] == 'healthy':
                   monitoring_results['status'] = 'warning'
           
           # Bellek metriƒüi
           SystemMetrics.objects.create(
               metric_name='memory_usage_percent',
               metric_type='gauge',
               value=memory.percent
           )
           
       except ImportError:
           logger.warning("‚ö†Ô∏è psutil y√ºkl√º deƒüil, bellek izlenemiyor")
       except Exception as e:
           logger.error(f"‚ùå Bellek izleme hatasƒ±: {e}")
       
       # Son 24 saatteki hata sayƒ±sƒ±
       try:
           last_24h = timezone.now() - timedelta(hours=24)
           error_count = ErrorLog.objects.filter(
               created_at__gte=last_24h,
               error_level__in=['ERROR', 'CRITICAL']
           ).count()
           
           monitoring_results['metrics']['errors_24h'] = error_count
           
           if error_count > 100:
               monitoring_results['alerts'].append({
                   'level': 'warning',
                   'message': f'Son 24 saatte {error_count} hata kaydedildi',
                   'action': 'error_investigation_needed'
               })
               if monitoring_results['status'] == 'healthy':
                   monitoring_results['status'] = 'warning'
           
           # Hata metriƒüi
           SystemMetrics.objects.create(
               metric_name='errors_24h',
               metric_type='gauge',
               value=error_count
           )
           
       except Exception as e:
           logger.error(f"‚ùå Hata sayƒ±sƒ± izleme hatasƒ±: {e}")
       
       # Celery queue uzunluƒüu
       try:
           from celery import current_app
           inspect = current_app.control.inspect()
           stats = inspect.stats()
           
           if stats:
               total_tasks = 0
               for worker_stats in stats.values():
                   total_tasks += worker_stats.get('total', {}).get('celery.tasks', 0)
               
               monitoring_results['metrics']['celery_queue_length'] = total_tasks
               
               if total_tasks > 1000:
                   monitoring_results['alerts'].append({
                       'level': 'warning',
                       'message': f'Celery kuyruk uzunluƒüu y√ºksek: {total_tasks}',
                       'action': 'queue_optimization_needed'
                   })
                   if monitoring_results['status'] == 'healthy':
                       monitoring_results['status'] = 'warning'
               
               # Queue metriƒüi
               SystemMetrics.objects.create(
                   metric_name='celery_queue_length',
                   metric_type='gauge',
                   value=total_tasks
               )
               
       except Exception as e:
           logger.error(f"‚ùå Celery izleme hatasƒ±: {e}")
       
       # Genel sistem durumu metriƒüi
       status_value = {
           'healthy': 1,
           'warning': 0.5,
           'critical': 0
       }.get(monitoring_results['status'], 0)
       
       SystemMetrics.objects.create(
           metric_name='system_status',
           metric_type='gauge',
           value=status_value,
           labels={'status': monitoring_results['status']}
       )
       
       logger.info(f"‚úÖ Sistem izleme tamamlandƒ± - Durum: {monitoring_results['status']}")
       
       if monitoring_results['alerts']:
           logger.warning(f"‚ö†Ô∏è {len(monitoring_results['alerts'])} uyarƒ± tespit edildi")
       
       return monitoring_results
       
   except Exception as exc:
       logger.error(f"‚ùå Sistem izleme hatasƒ±: {exc}")
       raise self.retry(exc=exc)


@shared_task(bind=True, max_retries=1)
def optimize_knowledge_chunks(self):
   """Knowledge chunk'larƒ± optimize et"""
   try:
       logger.info("üîß Knowledge chunk optimizasyonu ba≈ülƒ±yor...")
       
       optimization_results = {
           'chunks_processed': 0,
           'duplicates_removed': 0,
           'empty_chunks_removed': 0,
           'embeddings_regenerated': 0,
           'timestamp': timezone.now().isoformat()
       }
       
       # Bo≈ü chunk'larƒ± bul ve sil
       empty_chunks = KnowledgeChunk.objects.filter(
           models.Q(content__isnull=True) | 
           models.Q(content__exact='') |
           models.Q(content_length__lte=10)
       )
       empty_count = empty_chunks.count()
       empty_chunks.delete()
       optimization_results['empty_chunks_removed'] = empty_count
       
       if empty_count > 0:
           logger.info(f"‚úÖ {empty_count} bo≈ü chunk silindi")
       
       # Duplicate chunk'larƒ± bul (aynƒ± hash)
       from django.db.models import Count
       duplicate_hashes = KnowledgeChunk.objects.values('content_hash').annotate(
           count=Count('content_hash')
       ).filter(count__gt=1)
       
       duplicates_removed = 0
       for item in duplicate_hashes:
           content_hash = item['content_hash']
           chunks = KnowledgeChunk.objects.filter(content_hash=content_hash)
           
           # ƒ∞lkini sakla, diƒüerlerini sil
           first_chunk = chunks.first()
           duplicates = chunks.exclude(id=first_chunk.id)
           
           duplicate_count = duplicates.count()
           duplicates.delete()
           duplicates_removed += duplicate_count
       
       optimization_results['duplicates_removed'] = duplicates_removed
       
       if duplicates_removed > 0:
           logger.info(f"‚úÖ {duplicates_removed} duplicate chunk silindi")
       
       # Embedding'i olmayan chunk'larƒ± bul
       chunks_without_embedding = KnowledgeChunk.objects.filter(
           embedding__isnull=True
       )[:100]  # Toplu i≈üleme sƒ±nƒ±rƒ±
       
       embeddings_regenerated = 0
       for chunk in chunks_without_embedding:
           try:
               # Embedding'i yeniden olu≈ütur (bu i≈ülem ger√ßek implementasyonda
               # embedding service'e baƒülanacak)
               # chunk.embedding = generate_embedding(chunk.content)
               # chunk.save()
               embeddings_regenerated += 1
           except Exception as e:
               logger.warning(f"‚ö†Ô∏è Chunk {chunk.id} embedding hatasƒ±: {e}")
       
       optimization_results['embeddings_regenerated'] = embeddings_regenerated
       
       # ƒ∞statistikler
       total_chunks = KnowledgeChunk.objects.count()
       optimization_results['chunks_processed'] = total_chunks
       
       logger.info(f"‚úÖ Knowledge chunk optimizasyonu tamamlandƒ± - {total_chunks} chunk i≈ülendi")
       
       # Optimizasyon metriƒüi
       SystemMetrics.objects.create(
           metric_name='chunk_optimization',
           metric_type='counter',
           value=1,
           labels={
               'duplicates_removed': duplicates_removed,
               'empty_removed': empty_count,
               'embeddings_regenerated': embeddings_regenerated
           }
       )
       
       return optimization_results
       
   except Exception as exc:
       logger.error(f"‚ùå Chunk optimizasyon hatasƒ±: {exc}")
       raise self.retry(exc=exc)


@shared_task(bind=True)
def generate_maintenance_report(self):
   """Bakƒ±m raporu olu≈ütur"""
   try:
       logger.info("üìã Bakƒ±m raporu olu≈üturuluyor...")
       
       report_data = {
           'report_date': timezone.now().isoformat(),
           'system_health': {},
           'performance_metrics': {},
           'storage_stats': {},
           'user_activity': {},
           'error_summary': {},
           'recommendations': []
       }
       
       # Sistem saƒülƒ±ƒüƒ±
       try:
           health_result = system_health_check()
           report_data['system_health'] = health_result
       except Exception as e:
           logger.error(f"‚ùå Health check rapor hatasƒ±: {e}")
       
       # Son 7 g√ºn√ºn metrikleri
       try:
           last_week = timezone.now() - timedelta(days=7)
           
           # Performans metrikleri
           avg_response_time = PerformanceMetrics.objects.filter(
               timestamp__gte=last_week,
               metric_name='response_time'
           ).aggregate(models.Avg('value'))['value__avg'] or 0
           
           report_data['performance_metrics'] = {
               'avg_response_time_7d': round(avg_response_time, 2),
               'total_queries_7d': QueryAnalytics.objects.filter(
                   created_at__gte=last_week
               ).count(),
               'active_users_7d': QueryAnalytics.objects.filter(
                   created_at__gte=last_week
               ).values('user').distinct().count()
           }
           
       except Exception as e:
           logger.error(f"‚ùå Performans metrik rapor hatasƒ±: {e}")
       
       # Depolama istatistikleri
       try:
           total_documents = DocumentSource.objects.count()
           total_chunks = KnowledgeChunk.objects.count()
           processed_documents = DocumentSource.objects.filter(
               processing_status='completed'
           ).count()
           
           report_data['storage_stats'] = {
               'total_documents': total_documents,
               'total_chunks': total_chunks,
               'processed_documents': processed_documents,
               'processing_rate': round(
                   (processed_documents / total_documents * 100) if total_documents > 0 else 0, 1
               )
           }
           
       except Exception as e:
           logger.error(f"‚ùå Depolama istatistik hatasƒ±: {e}")
       
       # Hata √∂zeti
       try:
           last_24h = timezone.now() - timedelta(hours=24)
           
           error_counts = ErrorLog.objects.filter(
               created_at__gte=last_24h
           ).values('error_level').annotate(
               count=models.Count('id')
           )
           
           report_data['error_summary'] = {
               item['error_level']: item['count'] 
               for item in error_counts
           }
           
       except Exception as e:
           logger.error(f"‚ùå Hata √∂zeti rapor hatasƒ±: {e}")
       
       # √ñneriler
       recommendations = []
       
       # Disk kullanƒ±mƒ± kontrol√º
       if 'disk_usage_percent' in report_data.get('system_health', {}).get('metrics', {}):
           disk_usage = report_data['system_health']['metrics']['disk_usage_percent']
           if disk_usage > 80:
               recommendations.append({
                   'priority': 'high',
                   'category': 'storage',
                   'message': f'Disk kullanƒ±mƒ± %{disk_usage} - Temizlik √∂neriliyor',
                   'action': 'cleanup_old_data'
               })
       
       # Performans kontrol√º
       if report_data['performance_metrics'].get('avg_response_time_7d', 0) > 5:
           recommendations.append({
               'priority': 'medium',
               'category': 'performance',
               'message': 'Ortalama yanƒ±t s√ºresi y√ºksek - Optimizasyon gerekli',
               'action': 'optimize_database_indexes'
           })
       
       # Hata oranƒ± kontrol√º
       total_errors = sum(report_data.get('error_summary', {}).values())
       if total_errors > 50:
           recommendations.append({
               'priority': 'high',
               'category': 'stability',
               'message': f'Son 24 saatte {total_errors} hata - ƒ∞nceleme gerekli',
               'action': 'investigate_error_patterns'
           })
       
       report_data['recommendations'] = recommendations
       
       # Raporu dosya olarak kaydet
       import json
       report_dir = os.path.join(settings.MEDIA_ROOT, 'reports')
       os.makedirs(report_dir, exist_ok=True)
       
       timestamp = timezone.now().strftime('%Y%m%d_%H%M%S')
       report_file = os.path.join(report_dir, f'maintenance_report_{timestamp}.json')
       
       with open(report_file, 'w', encoding='utf-8') as f:
           json.dump(report_data, f, ensure_ascii=False, indent=2, default=str)
       
       logger.info(f"‚úÖ Bakƒ±m raporu olu≈üturuldu: {report_file}")
       
       # Rapor metriƒüi
       SystemMetrics.objects.create(
           metric_name='maintenance_report_generated',
           metric_type='counter',
           value=1,
           labels={
               'recommendations_count': len(recommendations),
               'file_path': report_file
           }
       )
       
       return {
           'success': True,
           'report_file': report_file,
           'recommendations_count': len(recommendations),
           'timestamp': timezone.now().isoformat()
       }
       
   except Exception as exc:
       logger.error(f"‚ùå Bakƒ±m raporu olu≈üturma hatasƒ±: {exc}")
       return {
           'success': False,
           'error': str(exc),
           'timestamp': timezone.now().isoformat()
       }


# Celery beat schedule i√ßin task listesi
MAINTENANCE_TASKS = {
   'daily-cleanup': {
       'task': 'sapbot_api.tasks.maintenance_tasks.daily_maintenance',
       'schedule': 3600.0 * 24,  # Her 24 saatte bir
       'options': {'queue': 'maintenance'}
   },
   'weekly-maintenance': {
       'task': 'sapbot_api.tasks.maintenance_tasks.weekly_maintenance',
       'schedule': 3600.0 * 24 * 7,  # Her hafta
       'options': {'queue': 'maintenance'}
   },
   'hourly-health-check': {
       'task': 'sapbot_api.tasks.maintenance_tasks.system_health_check',
       'schedule': 3600.0,  # Her saat
       'options': {'queue': 'monitoring'}
   },
   'resource-monitoring': {
       'task': 'sapbot_api.tasks.maintenance_tasks.monitor_system_resources',
       'schedule': 300.0,  # Her 5 dakika
       'options': {'queue': 'monitoring'}
   },
   'session-cleanup': {
       'task': 'sapbot_api.tasks.maintenance_tasks.cleanup_expired_sessions',
       'schedule': 1800.0,  # Her 30 dakika
       'options': {'queue': 'cleanup'}
   }
}